{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "# from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "\n",
    "# documents = [(list(movie_reviews.words(fileid)), category)\n",
    "#              for category in movie_reviews.categories()\n",
    "#              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "cantos_folder = 'texts/en/Inferno/'\n",
    "all_cantos = PlaintextCorpusReader(cantos_folder, '.*t')\n",
    "files = all_cantos.fileids()\n",
    "\n",
    "random.shuffle(files)\n",
    "\n",
    "print(files[1])\n",
    "\n",
    "all_words = []\n",
    "for w in files.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint, string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "\n",
    "# Remove punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "cantos_folder = 'texts/it/Inferno/'\n",
    "all_cantos = PlaintextCorpusReader(cantos_folder, '.*t')\n",
    "files = all_cantos.fileids()\n",
    "\n",
    "# print(files)\n",
    "\n",
    "for file in files:\n",
    "    with open (cantos_folder + file) as canto:\n",
    "        for line in canto:\n",
    "            print(word_tokenize(line, language='italian'))\n",
    "            print()\n",
    "            print(line)\n",
    "            thislist = tokenizer.tokenize(line)\n",
    "            print(thislist)\n",
    "            # If list is not empty, print last word only\n",
    "            if thislist:\n",
    "                print(thislist[-1])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint, string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "training_data = pd.read_csv('training_dataset.csv')\n",
    "training_data = training_data['Text']\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for line in training_data:\n",
    "    tokens = word_tokenize(line, language='italian')\n",
    "    filtered_words = filter(lambda token: token not in stopwords.words('italian'), tokens)\n",
    "    for item in filtered_words:\n",
    "        all_words = []\n",
    "for w in files.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))\n",
    "print(all_words[\"stupid\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
